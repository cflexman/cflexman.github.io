---
layout: post
date:   2017-04-29
title:  "How Not to Actually be Happy"
permalink: happy
categories: hedonics
comments: true
---

Main ways to subvert the happiness equation

One way to beat the treadmill is to go down on purpose. If your brain is pretty good at filtering out the "on purpose" parts of going up the treadmill (and this hypothesis seems backed up by what you might expect evolutionary hedonics to do), then can you instead filter out the bad parts? Boxing, ice baths, fasting, exercising the virtue of imperviousness, various physical training and pain, intentional isolation, etc---all seem potentially great at this. In these, the feeling of climbing back uphill is the part done to you by the environment, and the part going downhill is the part you subject yourself to. (An obvious way to ruin this technique for yourself is to overdwell on the regression to the mean portion.)

Another way to beat the treadmill is to step off and tie your happiness to some algorithm other than temporal-difference learning. Stoicism attempts to subvert the portion of your algorithm that is doing TD on environmental factors and replace it by virtue hedonics. Why would this work? Well, there's something weird about how TD applies less well to meta-stuff---you don't usually get tired of feeling love, you don't get tired of viewing yourself as strong, you don't get tired of feeling superior. (There are ways to take issue with this, but help steelman me here for a second.) If you just focus on the virtues you are displaying instead of the environmental reward, you may be able to maintain pretty constant positive self-image and significantly higher happiness set-point.

Another mutually exclusive option is to focus *more* on the environment. This would work by breaking the TD portion of you doing EV calcs, so you don't go outside expecting to be warm to get happy, but you go outside and then it is warm and then you are happy.

Other options include pessimism (reducing EV estimates by poor epistemics), premeditatio malorum (using availability heuristic to anchor S1 EV with fewer effects on S2), replacing the feeling of loss or bad happenstance with anger at the same thing ("The Man/patriarchy/corporate America is keeping me down; if it wasn't for them, ..."), replacing the feeling of loss or bad happenstance with anger at whatever is nearby, denial that the thing happened (unclear if this even occurs outside fin-de-siecle Vienna), and probably some others.

Another whole class of options involves convincing yourself that bad things aren't bad, but are actually good. This class tends to overlap significantly with the things we call "spirituality" to such an extent that I will use this term as shorthand here. Common spirituality methods abound, and a lot sound pretty similar, so you might think some of these should be lumped together but whatever. Some: "there is a good master plan and all bad things are part of that", the closely related "there is a benevolent master being and all bad things are still part of his will", "karma exists so all bad things are repaid in kind", "bad things are still so human and therefore interesting in a meta-sense", "we are stuck in a battle of good against evil and good will prevail so I can be optimistic about everything at long enough timescales", Nietsche's whole "everything is natural" shtick, eternal recurrence and the Norse rebirth of the world and really afterlife in general, "laughing at everything in life", "there is a seed of good in everyone/thing", "every setback is a chance for growth / pain is gain / / optimism," "when considering enough levels any statement can be funny and any outcome can ultimately be interesting", "something something deterministic trajectory toward optimistic Singularity solving everything and creating God", "all computable structures exist, so in one of them there is a near-maximally smart being simulating us and seeing what we see and knowing all the quirks of our mind as if some infatuated lover or benevolent father or cosmic friend".

A very related class is of all those options in which we convince ourselves that all events are petty and not meaningful: "we have divine significance and earthly things pale in comparison to God", "we have cosmic insignificance and earthly things pale in comparison to the Universe", "the only bad thing is suffering and you can free yourself by not caring", "IDGAF", nihilism and existentialism,

(Obviously a lot of these spirituality techniques are mutually exclusive. This explains why some people really like to e.g. stare at the stars and others don't)

A lot of these happiness-promoting spirituality techniques are directly at odds with truth. (Try telling someone who wants to pin all the bad things on overpaid corporate execs about how corporate innovation has saved their loved ones from brutal death and high CEO pay actually achieves several economic must-haves and anyways we all follow the blind tug of incentives and we can't re-design the incentive system well because everything is trade-offs.) This puts us in a hard situation when deciding how much truth to spread---so far, we've pretty much ignored this question and spread the truth everywhere we can, but I'm pretty unsure that this is the optimal strategy. To be discussed. (Best bet seems like it's to offer up replacement spiritualities less at odds with truth? Stoicism)

Did we miss anything? TD learning uses S1 and S2 to form EV estimates and compares them with the perceived outcome. We considered techniques that scrapped EV estimates; lowered EV estimates using S1 and S2; denied the outcome; replaced outcome losses with anger; made all outcomes seem good; made all outcomes go to 0 with no replacement in mind; and replaced outcomes with virtue. There are probably other ways to replace outcome losses than anger, and other ways to replace all outcomes than virtue.



-----

[mild info hazard; if you are great about not overthinking things or are pretty happy without resorting to tricks, don't read]

A bunch of rationalists try to make good decisions that maximize their utility. This is definitely the right thing to do if you're in business and want success. But I claim that the hedonic sap from applying this process in a naive way probably brings your mean down lower than you started. 

Standard advice runs like: it's not about how much money you have, it's about the deepness of your relationships (or something)! So it's easy to see this and say "oh, I need to spend more time with my friends."

But there may be a catch-22 here. Just like those in Unsong, where e.g. [spoilers] one cannot enter Hell for doing bad things if the goal of doing the bad things to get into Hell is actually for the greater good, or e.g. God will not grant your prayers for forgiveness if the motive of the prayers is to obtain the forgiveness God has to offer, similarly I claim that it is much harder to increase one's happiness when specifically focused on increasing happiness.

Your mind is really good at EV calcs, so when you spend a lot of time thinking about how to increase happiness and how much certain interventions will increase it, it makes sense that your mind will then have a higher EV for those interventions and thus TD learning will keep your outcome spread around 0. (This notably doesn't happen when people try things they expect to fail, and then they unexpectedly succeed.) So if you try to have deeper relationships with your friends to increase your happiness, it's pretty likely that your brain will know the EV is higher and outwit you: your relationships with your friends will get objectively better, but TD learning will keep you feeling about as you did before.

How have you ever been happy in the past then? If you know what a deep relationship is like and decide to have one of those again, your brain will know what to expect. You've been most happy when you didn't have the expectation down. So most really happy things fall into one of two categories: you actually haven't experienced an outcome like it before---the first time you fall in love---or you really just didn't expect the outcome---the randomly-met soul mate, the random night with friends that turns into the best thing, the random new thing you tried that turned out to be mesmerizingly joyful. In both of these classes, it genuinely hurts to have better ability to predict the future, and ignorance is rewarded.]

Just like the Unsong versions, the only way out seems to be somehow ending up with the same outcome without having that as a goal. That is definitely a difficult goal. One solution is to have someone else with approximately the same goal---in this case, they would set you up on awesome dates or would drag you along on great adventures without notice and without setting high expectations. Another solution is to make your policy something that leads you into situations with high hedonics, but isn't goal-oriented---an example is being guided by curiosity. Notice that this policy follows a virtue and not an expected outcome---the thing NOT to do here is to pursue sources with high expected value of information, because then you are doing most of the calculations your brain needs for the EV calc and just running on a slightly noisier treadmill.

So when reading or taking or forming or giving advice on how to be happy, I urge you not to leave it at just "gaining deeper relationships". The "deep relationships" outcome node is correlated with the "happiness" outcome node, but aiming at that node or any other upstream node as an intermediary to happiness will not capture the full signal. Committing a virtue that incidentally points toward "deep relationships" will capture the full potential. There are probably many virtues that do this: one is more radical honesty, one is making others happy, one is valuing all living things, one is curiosity. Have your pick, or start valuing many. Just making sure to *actually value* the virtues, not try to make others happy so they make you happy. 

------

Examples: okcupid finding good matches inflating expectations, searching for beautiful pictures on the internet, searching for pictures of beautiful actresses, searching for porn, searching for the best movies, knowing all the best places to go as a tourist in a new city, etc. The knowledge that statistically you are scrolling through all the best potential lovers changes the game (because your EV goes way up). If you swipe through hundreds of screwballs on Tinder before finding someone who shares many of your passions, you'll probably appreciate that person significantly more than someone you found on the "top matches" section of OkCupid. 



--------

This is probably not a problem with EA yet because the interventions are still pretty effective, but it could be in the future. Interventions where people "raise awareness" may actually make a bunch of people feel better (a sign in the coffee shop saying "refugees welcome"), while ones where some disease is eradicated or prevented to begin with may not give full returns on happiness. (Think of when you get a flu shot; is it ever more than an inconvenience? Even if you cerebrally appreciate it for a few seconds, are you ever like "damn this feels so viscerally good to not get the flu"?)

This is one rare criticism of EA that I don't think is entirely off the mark, though. When 80,000 Hours says that people are happier in altruistic jobs, and then urges people to take altruistic jobs for happiness, I think you are genuinely losing a good deal of the effect. It's actually hard to cheat your way into altruism. (This is almost like Newcomb-like behavior where you can't actually win by attempting locally to win, aside from all the differences).



------

othing to see here, [nothing at all][conspiracy]. Can you even see `this`?

[conspiracy]: http://www.thebayesianconspiracy.com/
